# M√≥dulo de Embeddings

## üåé Idioma / Language

- [Portugu√™s üáßüá∑](#portugu√™s)
- [English üá∫üá∏](#english)

---

<a name="portugu√™s"></a>
# Portugu√™s üáßüá∑

O m√≥dulo de embeddings do Synapstor fornece funcionalidades para converter texto em representa√ß√µes vetoriais (embeddings) que capturam o significado sem√¢ntico do conte√∫do. Estas representa√ß√µes s√£o essenciais para realizar pesquisas sem√¢nticas eficientes.

## Caracter√≠sticas Principais

- **M√∫ltiplos modelos suportados**: Integra√ß√£o com modelos populares de embeddings como os da OpenAI, Sentence Transformers e outros
- **Adaptadores flex√≠veis**: Arquitetura que permite adicionar facilmente novos modelos de embeddings
- **Gera√ß√£o em lote**: Processamento eficiente de m√∫ltiplos textos em uma √∫nica requisi√ß√£o
- **Caching inteligente**: Redu√ß√£o de custos e lat√™ncia atrav√©s de caching de embeddings anteriores
- **Detec√ß√£o autom√°tica**: Sele√ß√£o inteligente do modelo mais adequado com base no conte√∫do

## Arquitetura

O m√≥dulo √© organizado com os seguintes componentes:

### `embeddings_factory.py`

Implementa o padr√£o Factory para cria√ß√£o de inst√¢ncias de geradores de embeddings, permitindo selecionar dinamicamente o modelo mais adequado para cada caso de uso.

```python
from synapstor.embeddings import get_embeddings_generator

# Obter um gerador de embeddings usando o modelo padr√£o
generator = get_embeddings_generator()

# Ou especificar um modelo particular
generator = get_embeddings_generator(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

### `base_generator.py`

Define a interface comum (`BaseEmbeddingsGenerator`) que todos os geradores de embeddings devem implementar, garantindo consist√™ncia entre diferentes implementa√ß√µes.

### Adaptadores de Modelo

- `openai_generator.py`: Integra√ß√µes com modelos da OpenAI
- `st_generator.py`: Integra√ß√µes com modelos Sentence Transformers
- `huggingface_generator.py`: Integra√ß√µes com modelos do Hugging Face

## Exemplo de Uso

```python
from synapstor.embeddings import get_embeddings_generator

# Inicializar o gerador de embeddings
embedding_generator = get_embeddings_generator(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    cache_dir="./cache"
)

# Gerar embeddings para um √∫nico texto
texto = "O Synapstor √© uma ferramenta para armazenamento e pesquisa sem√¢ntica de c√≥digo"
embedding = embedding_generator.get_embeddings(texto)

# Gerar embeddings para m√∫ltiplos textos
textos = [
    "Pesquisa sem√¢ntica de c√≥digo",
    "Armazenamento de conhecimento",
    "Assistente de programa√ß√£o"
]
embeddings = embedding_generator.get_batch_embeddings(textos)

# Verificar a dimensionalidade
print(f"Dimens√£o do embedding: {len(embedding)}")
```

## Configura√ß√£o Avan√ßada

### Cache de Embeddings

Para evitar recalcular embeddings j√° gerados anteriormente:

```python
from synapstor.embeddings import get_embeddings_generator

# Ativar cache para economizar recursos
generator = get_embeddings_generator(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    use_cache=True,
    cache_dir="./embeddings_cache"
)

# Os embeddings ser√£o armazenados localmente
# e reutilizados em chamadas futuras com os mesmos textos
```

### Normaliza√ß√£o de Embeddings

Normaliza√ß√£o para garantir consist√™ncia nas opera√ß√µes de similaridade:

```python
from synapstor.embeddings import get_embeddings_generator

# Ativar normaliza√ß√£o autom√°tica (L2)
generator = get_embeddings_generator(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    normalize=True
)

# Todos os embeddings gerados ter√£o norma = 1
```

## Sele√ß√£o de Modelo

Recomenda√ß√µes de modelos para diferentes casos de uso:

- **Uso geral**: `sentence-transformers/all-MiniLM-L6-v2` (padr√£o)
- **Portugu√™s**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Precis√£o m√°xima**: `sentence-transformers/all-mpnet-base-v2`
- **Velocidade m√°xima**: `sentence-transformers/all-MiniLM-L3-v2`

## Desempenho e Otimiza√ß√£o

T√©cnicas para otimizar o uso de recursos:

- Use `get_batch_embeddings()` para processar m√∫ltiplos textos de uma vez
- Ative o cache para textos frequentes ou para economizar chamadas de API
- Selecione modelos menores quando a velocidade for mais importante que a precis√£o
- Execute localmente modelos Sentence Transformers para evitar custos de API

---

<a name="english"></a>
# English üá∫üá∏

The Synapstor embeddings module provides functionality to convert text into vector representations (embeddings) that capture the semantic meaning of content. These representations are essential for performing efficient semantic searches.

## Main Features

- **Multiple supported models**: Integration with popular embedding models such as OpenAI, Sentence Transformers, and others
- **Flexible adapters**: Architecture that allows easy addition of new embedding models
- **Batch generation**: Efficient processing of multiple texts in a single request
- **Intelligent caching**: Reduction of costs and latency through caching of previous embeddings
- **Automatic detection**: Intelligent selection of the most suitable model based on content

## Architecture

The module is organized with the following components:

### `embeddings_factory.py`

Implements the Factory pattern for creating instances of embedding generators, allowing dynamic selection of the most suitable model for each use case.

```python
from synapstor.embeddings import get_embeddings_generator

# Get an embeddings generator using the default model
generator = get_embeddings_generator()

# Or specify a particular model
generator = get_embeddings_generator(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

### `base_generator.py`

Defines the common interface (`BaseEmbeddingsGenerator`) that all embedding generators must implement, ensuring consistency across different implementations.

### Model Adapters

- `openai_generator.py`: Integrations with OpenAI models
- `st_generator.py`: Integrations with Sentence Transformers models
- `huggingface_generator.py`: Integrations with Hugging Face models

## Usage Example

```python
from synapstor.embeddings import get_embeddings_generator

# Initialize the embeddings generator
embedding_generator = get_embeddings_generator(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    cache_dir="./cache"
)

# Generate embeddings for a single text
text = "Synapstor is a tool for storing and semantically searching code"
embedding = embedding_generator.get_embeddings(text)

# Generate embeddings for multiple texts
texts = [
    "Semantic code search",
    "Knowledge storage",
    "Programming assistant"
]
embeddings = embedding_generator.get_batch_embeddings(texts)

# Check dimensionality
print(f"Embedding dimension: {len(embedding)}")
```

## Advanced Configuration

### Embeddings Cache

To avoid recalculating previously generated embeddings:

```python
from synapstor.embeddings import get_embeddings_generator

# Enable caching to save resources
generator = get_embeddings_generator(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    use_cache=True,
    cache_dir="./embeddings_cache"
)

# Embeddings will be stored locally
# and reused in future calls with the same texts
```

### Embeddings Normalization

Normalization to ensure consistency in similarity operations:

```python
from synapstor.embeddings import get_embeddings_generator

# Enable automatic normalization (L2)
generator = get_embeddings_generator(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    normalize=True
)

# All generated embeddings will have norm = 1
```

## Model Selection

Recommended models for different use cases:

- **General use**: `sentence-transformers/all-MiniLM-L6-v2` (default)
- **Portuguese**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Maximum accuracy**: `sentence-transformers/all-mpnet-base-v2`
- **Maximum speed**: `sentence-transformers/all-MiniLM-L3-v2`

## Performance and Optimization

Techniques to optimize resource usage:

- Use `get_batch_embeddings()` to process multiple texts at once
- Enable caching for frequent texts or to save API calls
- Select smaller models when speed is more important than accuracy
- Run Sentence Transformers models locally to avoid API costs
